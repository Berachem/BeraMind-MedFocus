from flask import Flask, render_template, request, jsonify, url_for
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import base64
import os
from PIL import Image
import io
import json
from datetime import datetime
import threading

app = Flask(__name__)
app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # 16MB max file size
app.config['UPLOAD_FOLDER'] = 'static/uploads'

# Configuration Transformers
MODEL_NAME = os.getenv('MODEL_NAME', 'google/medgemma-4b-it')
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
MODEL_LOADED = False
model = None
tokenizer = None
text_generator = None

# Cr√©er le dossier uploads s'il n'existe pas
os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)

def load_model():
    """Charge le mod√®le MedGemma avec Transformers"""
    global model, tokenizer, text_generator, MODEL_LOADED
    
    try:
        print(f"üîÑ Chargement du mod√®le {MODEL_NAME} sur {DEVICE}...")
        
        # Charger le tokenizer
        tokenizer = AutoTokenizer.from_pretrained(
            MODEL_NAME,
            trust_remote_code=True
        )
        
        # Configuration du mod√®le selon le device
        model_kwargs = {
            "trust_remote_code": True,
            "torch_dtype": torch.float16 if DEVICE == "cuda" else torch.float32,
        }
        
        if DEVICE == "cuda":
            model_kwargs["device_map"] = "auto"
        
        # Charger le mod√®le
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_NAME,
            **model_kwargs
        )
        
        if DEVICE == "cpu":
            model = model.to(DEVICE)
        
        # Cr√©er le pipeline de g√©n√©ration de texte
        text_generator = pipeline(
            "text-generation",
            model=model,
            tokenizer=tokenizer,
            device=0 if DEVICE == "cuda" else -1,
            do_sample=True,
            temperature=0.7,
            max_new_tokens=500,
            pad_token_id=tokenizer.eos_token_id
        )
        
        MODEL_LOADED = True
        print(f"‚úÖ Mod√®le {MODEL_NAME} charg√© avec succ√®s sur {DEVICE}")
        
    except Exception as e:
        MODEL_LOADED = False
        print(f"‚ùå Erreur lors du chargement du mod√®le: {str(e)}")

# Charger le mod√®le au d√©marrage dans un thread s√©par√©
print("üöÄ D√©marrage de l'application MedExplain...")
model_thread = threading.Thread(target=load_model)
model_thread.start()

def is_medical_emergency(text):
    """D√©tecte les urgences m√©dicales dans le texte"""
    emergency_keywords = [
        'infarctus', 'crise cardiaque', 'avc', 'accident vasculaire',
        'douleur thoracique', 'essoufflement s√©v√®re', 'perte de conscience',
        'convulsions', 'h√©morragie', 'fracture ouverte', 'urgence'
    ]
    
    text_lower = text.lower()
    for keyword in emergency_keywords:
        if keyword in text_lower:
            return True
    return False

def generate_medical_response(question, is_image_analysis=False):
    """G√©n√®re une r√©ponse m√©dicale avec Transformers"""
    if not MODEL_LOADED or text_generator is None:
        return "‚è≥ Le mod√®le est en cours de chargement. Veuillez patienter quelques instants et r√©essayer."
    
    try:
        # Pr√©parer le prompt m√©dical
        system_prompt = """Tu es un assistant m√©dical IA √©ducatif bas√© sur MedGemma. 
        Fournis des informations m√©dicales pr√©cises et √©ducatives uniquement.
        Rappelle toujours de consulter un professionnel de sant√© qualifi√©.
        R√©ponds en fran√ßais de mani√®re claire et structur√©e."""
        
        if is_image_analysis:
            system_prompt += "\nTu analyses une image m√©dicale. D√©cris ce que tu observes de mani√®re √©ducative."
        
        prompt = f"{system_prompt}\n\nQuestion: {question}\n\nR√©ponse:"
        
        # G√©n√©rer la r√©ponse
        response = text_generator(
            prompt,
            max_new_tokens=400,
            do_sample=True,
            temperature=0.7,
            top_p=0.95,
            repetition_penalty=1.1
        )
        
        # Extraire seulement la nouvelle partie g√©n√©r√©e
        generated_text = response[0]['generated_text']
        answer = generated_text.split("R√©ponse:")[-1].strip()
        
        # Ajouter un avertissement m√©dical
        medical_disclaimer = "\n\n‚ö†Ô∏è **Avertissement m√©dical**: Cette information est fournie √† des fins √©ducatives uniquement. En cas d'urgence, contactez imm√©diatement les services d'urgence (15 ou 112). Consultez toujours un professionnel de sant√© qualifi√© pour un diagnostic et un traitement appropri√©s."
        
        return answer + medical_disclaimer
        
    except Exception as e:
        return f"‚ùå Erreur lors de la g√©n√©ration de la r√©ponse: {str(e)}"

@app.route('/')
def index():
    """Page d'accueil"""
    return render_template('index.html')

@app.route('/chat')
def chat():
    """Interface de chat m√©dical"""
    return render_template('chat.html')

@app.route('/image')
def image_analysis():
    """Interface d'analyse d'images"""
    return render_template('image.html')

@app.route('/ask', methods=['POST'])
def ask_question():
    """Endpoint pour les questions textuelles"""
    try:
        data = request.get_json()
        question = data.get('question', '')
        
        if not question:
            return jsonify({'error': 'Question manquante'}), 400
        
        # G√©n√©rer la r√©ponse avec Transformers
        answer = generate_medical_response(question)
        
        # D√©tection d'urgence
        is_emergency = is_medical_emergency(question + " " + answer)
        
        return jsonify({
            'answer': answer,
            'is_emergency': is_emergency,
            'model': MODEL_NAME,
            'device': DEVICE,
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        return jsonify({'error': f'Erreur serveur: {str(e)}'}), 500

@app.route('/analyze_image', methods=['POST'])
def analyze_image():
    """Endpoint pour l'analyse d'images m√©dicales"""
    try:
        if 'image' not in request.files:
            return jsonify({'error': 'Aucune image fournie'}), 400
        
        file = request.files['image']
        question = request.form.get('question', 'Analysez cette image m√©dicale et d√©crivez ce que vous observez.')
        
        if file.filename == '':
            return jsonify({'error': 'Aucun fichier s√©lectionn√©'}), 400
        
        # Sauvegarder l'image
        filename = f"medical_{datetime.now().strftime('%Y%m%d_%H%M%S')}.jpg"
        filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)
        file.save(filepath)
        
        # Pour cette version, on analyse le texte de la question
        # Note: MedGemma ne supporte pas nativement les images, on se base sur la description
        image_question = f"Analyse d'image m√©dicale demand√©e: {question}. Fournissez des conseils g√©n√©raux d'interpr√©tation d'images m√©dicales."
        
        # G√©n√©rer la r√©ponse avec Transformers
        answer = generate_medical_response(image_question, is_image_analysis=True)
        
        # D√©tection d'urgence
        is_emergency = is_medical_emergency(question + " " + answer)
        
        return jsonify({
            'answer': answer,
            'image_url': url_for('static', filename=f'uploads/{filename}'),
            'is_emergency': is_emergency,
            'model': MODEL_NAME,
            'device': DEVICE,
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        return jsonify({'error': f'Erreur serveur: {str(e)}'}), 500

@app.route('/health')
def health_check():
    """V√©rification de l'√©tat de l'application"""
    return jsonify({
        'flask': True,
        'transformers': MODEL_LOADED,
        'model': MODEL_NAME,
        'device': DEVICE,
        'cuda_available': torch.cuda.is_available(),
        'timestamp': datetime.now().isoformat(),
        'status': 'healthy' if MODEL_LOADED else 'loading'
    })

if __name__ == '__main__':
    print("üåê Application Flask d√©marr√©e sur http://localhost:5000")
    print("üìö Interface chat: http://localhost:5000/chat")
    print("üñºÔ∏è Analyse d'images: http://localhost:5000/image")
    print("‚ù§Ô∏è √âtat de sant√©: http://localhost:5000/health")
    app.run(debug=True, host='0.0.0.0', port=5000)
